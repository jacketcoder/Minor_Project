{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import os,json\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class convertCVtoText:\n",
    "    @staticmethod\n",
    "    def startConversion(fileName):\n",
    "        pdfFileObj = open(fileName,'rb')     #'rb' for read binary mode\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        noOfPages=pdfReader.numPages\n",
    "        text=\"\"\n",
    "        for pages in range(0,pdfReader.numPages):\n",
    "            pageObj = pdfReader.getPage(pages)          #'9' is the page number\n",
    "            text+=pageObj.extractText()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class textCleaner:\n",
    "    def __init__(self):\n",
    "        self.text=\"\"\n",
    "        self.cleanText=\"\"\n",
    "    def normalizeText(self):\n",
    "        norText=\"\"\n",
    "        returnText=\"\"\n",
    "        norText+= re.sub(r'[^a-zA-Z ]',r' ',self.text)\n",
    "        returnText+=re.sub(' +',' ',norText)\n",
    "        self.cleanText+=re.sub(r'([A-Z])', lambda pat: pat.group(1).lower(), returnText)\n",
    "       \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CV:\n",
    "     def __init__(self,name,path,post):\n",
    "            self.fileName=name\n",
    "            self.filePath=path\n",
    "            self.CVCategory=post\n",
    "            self.textHandeller=textCleaner()\n",
    "            self.textHandeller.text=convertCVtoText.startConversion(self.filePath)\n",
    "            self.textHandeller.normalizeText()\n",
    "            self.featureVector=[]\n",
    "            self.score=None\n",
    "            self.frequencyVector=[]\n",
    "            \n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from skfeature.utility.util import reverse_argsort\n",
    "class refiefFAlgo:\n",
    "    def __init__(self,mode=None):\n",
    "        self.scoreList=[]\n",
    "        self.wordIndex=[]\n",
    "        self.mode=mode\n",
    "    def feature_ranking(self):\n",
    "            \"\"\"\n",
    "            Rank features in descending order according to reliefF score, the higher the reliefF score, the more important the\n",
    "            feature is\n",
    "            \"\"\"\n",
    "            for scorePerCluster in self.scoreList:\n",
    "                temp=np.asarray(scorePerCluster) \n",
    "                idx = np.argsort(temp, 0)\n",
    "            #print(idx)\n",
    "            self.wordIndex.append(idx[::-1])\n",
    "            #return idx[::-1]\n",
    "    def score_ranking(self):\n",
    "            \"\"\"\n",
    "            Rank features in descending order according to reliefF score, the higher the reliefF score, the more important the\n",
    "            feature is\n",
    "            \"\"\"\n",
    "            for scorePerCluster in self.scoreList:\n",
    "                temp=np.asarray(scorePerCluster) \n",
    "                idx = np.argsort(temp, 0)\n",
    "            #print(idx)\n",
    "            self.wordIndex.append(idx[::-1])\n",
    "            #return idx[::-1]\n",
    "    def reliefF(self,X, y,**kwargs):\n",
    "        \"\"\"\n",
    "        This function implements the reliefF feature selection\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        X: {numpy array}, shape (n_samples, n_features)\n",
    "            input data\n",
    "        y: {numpy array}, shape (n_samples,)\n",
    "            input class labels\n",
    "        kwargs: {dictionary}\n",
    "            parameters of reliefF:\n",
    "            k: {int}\n",
    "                choices for the number of neighbors (default k = 5)\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        score: {numpy array}, shape (n_features,)\n",
    "            reliefF score for each feature\n",
    "\n",
    "        Reference\n",
    "        ---------\n",
    "        Robnik-Sikonja, Marko et al. \"Theoretical and empirical analysis of relieff and rrelieff.\" Machine Learning 2003.\n",
    "        Zhao, Zheng et al. \"On Similarity Preserving Feature Selection.\" TKDE 2013.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if \"k\" not in list(kwargs.keys()):\n",
    "            k = 5\n",
    "        else:\n",
    "            k = kwargs[\"k\"]\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # calculate pairwise distances between instances\n",
    "        distance = pairwise_distances(X, metric='manhattan')\n",
    "\n",
    "\n",
    "\n",
    "        # the number of sampled instances is equal to the number of total instances\n",
    "        for idx in range(n_samples):\n",
    "            score = np.zeros(n_features)\n",
    "            near_hit = []\n",
    "            near_miss = dict()\n",
    "\n",
    "            self_fea = X[idx, :]\n",
    "            c = np.unique(y).tolist()\n",
    "\n",
    "            stop_dict = dict()\n",
    "            for label in c:\n",
    "                stop_dict[label] = 0\n",
    "            del c[c.index(y[idx])]\n",
    "\n",
    "            p_dict = dict()\n",
    "            p_label_idx = float(len(y[y == y[idx]]))/float(n_samples)\n",
    "\n",
    "            for label in c:\n",
    "                p_label_c = float(len(y[y == label]))/float(n_samples)\n",
    "                p_dict[label] = p_label_c/(1-p_label_idx)\n",
    "                near_miss[label] = []\n",
    "\n",
    "            distance_sort = []\n",
    "            distance[idx, idx] = np.max(distance[idx, :])\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                distance_sort.append([distance[idx, i], int(i), y[i]])\n",
    "            distance_sort.sort(key=lambda x: x[0])\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                # find k nearest hit points\n",
    "                if distance_sort[i][2] == y[idx]:\n",
    "                    if len(near_hit) < k:\n",
    "                        near_hit.append(distance_sort[i][1])\n",
    "                    elif len(near_hit) == k:\n",
    "                        stop_dict[y[idx]] = 1\n",
    "                else:\n",
    "                    # find k nearest miss points for each label\n",
    "                    if len(near_miss[distance_sort[i][2]]) < k:\n",
    "                        near_miss[distance_sort[i][2]].append(distance_sort[i][1])\n",
    "                    else:\n",
    "                        if len(near_miss[distance_sort[i][2]]) == k:\n",
    "                            stop_dict[distance_sort[i][2]] = 1\n",
    "                stop = True\n",
    "                for (key, value) in list(stop_dict.items()):\n",
    "                        if value != 1:\n",
    "                            stop = False\n",
    "                if stop:\n",
    "                    break\n",
    "\n",
    "            # update reliefF score\n",
    "            near_hit_term = np.zeros(n_features)\n",
    "            for ele in near_hit:\n",
    "                near_hit_term = np.array(abs(self_fea-X[ele, :]))+np.array(near_hit_term)\n",
    "\n",
    "            near_miss_term = dict()\n",
    "            for (label, miss_list) in list(near_miss.items()):\n",
    "                near_miss_term[label] = np.zeros(n_features)\n",
    "                for ele in miss_list:\n",
    "                    near_miss_term[label] = np.array(abs(self_fea-X[ele, :]))+np.array(near_miss_term[label])\n",
    "                score += near_miss_term[label]/(k*p_dict[label])\n",
    "            score -= near_hit_term/k\n",
    "            self.scoreList.append(score)\n",
    "            self.feature_ranking()\n",
    "        if self.mode == 'raw':\n",
    "            print(\"here\")\n",
    "            print(score)\n",
    "            return score\n",
    "        elif self.mode == 'index':\n",
    "            print(\"herew\")\n",
    "            return feature_ranking(score)\n",
    "        elif self.mode == 'rank':\n",
    "            print(\"hereq\")\n",
    "            return reverse_argsort(feature_ranking(score), X.shape[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CBRAlgo:\n",
    "    def __init__(self):\n",
    "        self.CVScoreList=[]\n",
    "        self.topWords=None\n",
    "        self.clusterWiseTopWordList=[]\n",
    "        self.overAllWeight=[]\n",
    "    def getTopWords(self,vocabulary,clusterCenters,noOfFormedClusters):\n",
    "        x=[]\n",
    "        for key,values in vocabulary.items():\n",
    "            x.append(values)  \n",
    "        self.topWords=refiefFAlgo()\n",
    "        self.topWords.reliefF(clusterCenters,np.asarray(x),k=noOfFormedClusters-1)\n",
    "        \n",
    "    def getOverallWeightOfRelevantWords(self):\n",
    "        self.overAllWeight=np.average([x for x in self.topWords.scoreList],axis=0)\n",
    "        \n",
    "    def calculateCVScoreViaCluster(self,documentMatrix,vocabulary,clusteringInfo):\n",
    "        self.getTopWords(vocabulary,clusteringInfo.kMeans.cluster_centers_,clusteringInfo.bestClusterToForm)\n",
    "        self.getTopWordsPerCluster(clusteringInfo.kMeans.cluster_centers_,vocabulary)\n",
    "        self.getOverallWeightOfRelevantWords()\n",
    "        featureVector=documentMatrix.toarray()\n",
    "        for cvNumber,clusterNumber in enumerate(clusteringInfo.kMeans.labels_):\n",
    "            score=0\n",
    "            for wordFrequency,weight in zip(featureVector[cvNumber],self.topWords.scoreList[clusterNumber]):\n",
    "                score+=wordFrequency*weight\n",
    "            self.CVScoreList.append(score)    \n",
    "            \n",
    "    def calculateCVScore(self,documentMatrix,vocabulary,clusteringInfo):\n",
    "        self.getTopWords(vocabulary,clusteringInfo.kMeans.cluster_centers_,clusteringInfo.bestClusterToForm)\n",
    "        self.getTopWordsPerCluster(clusteringInfo.kMeans.cluster_centers_,vocabulary)\n",
    "        self.getOverallWeightOfRelevantWords()\n",
    "        featureVector=documentMatrix.toarray()\n",
    "        for cvNumber,clusterNumber in enumerate(clusteringInfo.kMeans.labels_):\n",
    "            score=0\n",
    "            for wordFrequency,weight in zip(featureVector[cvNumber],self.overAllWeight):\n",
    "                score+=wordFrequency*weight\n",
    "            self.CVScoreList.append(score)    \n",
    "            \n",
    "    def getTopWordsPerCluster(self,clusterCenters,vocabulary):\n",
    "        for clusterNo,impFeaturesRow in enumerate(clusterCenters):\n",
    "            WordList={}\n",
    "            for indexNo in self.topWords.wordIndex[clusterNo]:\n",
    "                WordList.update({list(vocabulary.keys())[list(vocabulary.values()).index(indexNo)]:self.topWords.scoreList[clusterNo][indexNo]})\n",
    "            self.clusterWiseTopWordList.append(WordList)\n",
    "    def plotTopWordsPerCluster(self):\n",
    "        topwords=10\n",
    "        width =1\n",
    "        import matplotlib.pyplot as plt\n",
    "        for index,clusterTopWord in enumerate(self.clusterWiseTopWordList):\n",
    "            ig,ax = plt.subplots()\n",
    "            lists = [(key,value) for (key,value) in clusterTopWord.items()] # sorted by key, return a list of tuples\n",
    "            key, value = zip(*lists)\n",
    "            x = np.arange(topwords)\n",
    "            plt.barh(x[:topwords],value[:topwords],align='center')\n",
    "            plt.yticks(x, key[:topwords])\n",
    "            plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "            plt.title('top %d words in %d cluster'%(topwords,index))\n",
    "            plt.ylabel('words')\n",
    "            plt.xlabel('weight')\n",
    "            plt.show()\n",
    "    def plotOverAllWeight(self,vocabulary):\n",
    "        import matplotlib.pyplot as pl\n",
    "        ig,ax = plt.subplots()\n",
    "        x = np.arange(0,len(self.overAllWeight))\n",
    "        pl.barh(x,self.overAllWeight,align='center')\n",
    "        pl.yticks(x,vocabulary)\n",
    "        pl.rcParams[\"figure.figsize\"] = (20,20)\n",
    "        pl.title('weight bar graph of Relevant words')\n",
    "        pl.ylabel('words')\n",
    "        pl.xlabel('weight')          \n",
    "        pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLTKHelper:\n",
    "    def __init__(self):\n",
    "        self.documentMatrix=None\n",
    "        self.vocabulary=None\n",
    "        self.normalizedFeatureSet=[]\n",
    "    def findDocumentMatrix(self,totalCVText,minFrequency,vocab):\n",
    "        #vectorizer=CountVectorizer(stop_words='english',min_df=minFrequency)\n",
    "        vectorizer=CountVectorizer(stop_words='english',vocabulary=vocab)\n",
    "        #vectorizer=CountVectorizer(stop_words='english')\n",
    "        self.documentMatrix=vectorizer.fit_transform(totalCVText)\n",
    "        self.vocabulary=vectorizer.vocabulary_\n",
    "        #return documentMatrix,vocabulary \n",
    "        self.normalizeMatrix()\n",
    "    def normalizeMatrix(self):\n",
    "        self.normalizedFeatureSet=normalize(self.documentMatrix.toarray().astype('float64'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Clustering:\n",
    "    def __init__(self):\n",
    "        self.kMeans=None\n",
    "        self.bestClusterToForm=None\n",
    "        self.silCoeffInfo={}\n",
    "        self.minCluster=2\n",
    "        self.maxCluster=10\n",
    "    def findSilCoeff(self,data):\n",
    "        for n_cluster in range(self.minCluster, self.maxCluster):\n",
    "            kmeans = KMeans(n_clusters=n_cluster).fit(data)\n",
    "            label = kmeans.labels_\n",
    "            sil_coeff = silhouette_score(data, label, metric='euclidean')\n",
    "            self.silCoeffInfo.update({n_cluster:float(sil_coeff)})\n",
    "        maxSilCoeff=max(self.silCoeffInfo.values())\n",
    "        maxSilCoeffkeys = [k for k, v in self.silCoeffInfo.items() if v == maxSilCoeff]\n",
    "        if(len(maxSilCoeffkeys)==1):\n",
    "            for x in maxSilCoeffkeys:\n",
    "                self.bestClusterToForm=x\n",
    "        else:\n",
    "            print(\"2 keys,confusion\")\n",
    "    def clusterData(self,data):\n",
    "        self.findSilCoeff(data)\n",
    "        self.kMeans=KMeans(n_clusters=self.bestClusterToForm).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CVManager:\n",
    "    def __init__(self):\n",
    "        self.CVList=[]\n",
    "        self.cvsFile=\"documentMatrix.csv\"\n",
    "        self.CVFileName=[]\n",
    "        self.fileNamesWithPath=[]\n",
    "        self.cvPostList=[]\n",
    "        self.CVTextColl=[]\n",
    "        self.noOfTopCV=10\n",
    "        self.orderedCVList=[]\n",
    "        self.languageProcessing=None\n",
    "        self.clusteringInfo=None\n",
    "        self.CVRanker=None\n",
    "    def list_CVs(self,rootPath):\n",
    "          \n",
    "        for root, dirs, files in os.walk(rootPath):\n",
    "            for name in files:\n",
    "                self.CVFileName.append(name)\n",
    "                self.fileNamesWithPath.append(os.path.join(root, name))\n",
    "                self.cvPostList.append(os.path.basename(os.path.dirname(os.path.join(root,name))))\n",
    "               \n",
    "    def collectCV(self):\n",
    "            for cvFilePath,cvFileName,cvPost in zip(self.fileNamesWithPath,self.CVFileName,self.cvPostList):\n",
    "                try:\n",
    "                    newCV=CV(cvFileName,cvFilePath,cvPost)\n",
    "                    self.CVList.append(newCV)\n",
    "                except Exception as e:\n",
    "                    print(cvFileName)\n",
    "                    print(\"in collection of CV \\t\"+str(e))\n",
    "            \n",
    "    def collectCVText(self):\n",
    "        self.CVTextColl=[]\n",
    "        for cv in self.CVList:\n",
    "            self.CVTextColl.append(cv.textHandeller.cleanText) \n",
    "    def findDocumentMatrix(self,minFrequency,vocab):\n",
    "            self.collectCVText()\n",
    "            self.languageProcessing=NLTKHelper()\n",
    "            self.languageProcessing.findDocumentMatrix(self.CVTextColl,minFrequency,vocab)\n",
    "            self.assignFeatureVector()  \n",
    "    def assignFeatureVector(self):\n",
    "        for cv,cvNum in zip(self.CVList,range(len(self.CVFileName))):\n",
    "            cv.featureVector=self.languageProcessing.normalizedFeatureSet[cvNum]\n",
    "        for cv,cvNum in zip(self.CVList,range(len(self.CVFileName))):\n",
    "            cv.frequencyVector=self.languageProcessing.documentMatrix.toarray()[cvNum]\n",
    "            #frequencyVector\n",
    "    def makeGraph(self,data):\n",
    "        lists = sorted(data) # sorted by key, return a list of tuples\n",
    "        x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "        plt.plot(x, y)\n",
    "        plt.show()\n",
    "        \n",
    "    def clusterData(self):\n",
    "        self.clusteringInfo=Clustering()\n",
    "        self.clusteringInfo.clusterData(self.languageProcessing.normalizedFeatureSet)\n",
    "        self.makeGraph(self.clusteringInfo.silCoeffInfo.items())\n",
    "    def rankCV(self):\n",
    "        self.CVRanker=CBRAlgo()\n",
    "        self.CVRanker.calculateCVScore(self.languageProcessing.documentMatrix,self.languageProcessing.vocabulary,self.clusteringInfo)\n",
    "        for cv,cvScore in zip(self.CVList,self.CVRanker.CVScoreList):\n",
    "            cv.score=cvScore\n",
    "    def showAnalytics(self):\n",
    "        self.CVRanker.plotTopWordsPerCluster()\n",
    "        self.CVRanker.plotOverAllWeight(self.languageProcessing.vocabulary)\n",
    "    def showTopCVPerPost(self,post):\n",
    "        cvlist={}\n",
    "        cvScore=[]\n",
    "        cvData=[]\n",
    "        if post is None:\n",
    "            for cvCategery in set(self.cvPostList):\n",
    "                cvlist={}\n",
    "                print(\"cv of %s\"%cvCategery)\n",
    "                for cv in self.CVList:\n",
    "                    if(cv.CVCategory==cvCategery):\n",
    "                        cvlist.update({cv:cv.score})\n",
    "                temp=[(value,key) for key,value in cvlist.items()]\n",
    "                temp.sort()\n",
    "                temp.reverse()\n",
    "                temp=[(key,value) for value,key in temp]\n",
    "                cvData=temp\n",
    "                return cvData\n",
    "        else:\n",
    "            try:\n",
    "                print(\"cv of post %s\"%post)\n",
    "                for cv in self.CVList:\n",
    "                    if(cv.CVCategory==post):\n",
    "                        cvlist.update({cv:cv.score})\n",
    "                temp=[(value,key) for key,value in cvlist.items()]\n",
    "                temp.sort()\n",
    "                temp.reverse()\n",
    "                temp=[(key,value) for value,key in temp]\n",
    "                cvData=temp\n",
    "                return cvData\n",
    "            except Exception as e:\n",
    "                    print(cv.fileName)\n",
    "                    print(\"finding top CV \\t\"+str(e))\n",
    "            \n",
    "    def compareCV(self):\n",
    "        topCVNum=2\n",
    "        cvIndex=0\n",
    "        #print(self.orderedCVList)\n",
    "        for cv in self.orderedCVList[:topCVNum]:\n",
    "            xValue = np.arange(len(cv[cvIndex].frequencyVector))\n",
    "            plt.plot(xValue,cv[cvIndex].frequencyVector, markersize = 10,label=cv[cvIndex].fileName+'='+str(cv[cvIndex].score))\n",
    "            #plt.rcParams[\"figure.figsize\"] = (10,20)\n",
    "        plt.title('CV comparision top cv')\n",
    "        plt.ylabel('Feature Vector Frequency')\n",
    "        plt.legend(mode=\"expand\")\n",
    "        plt.xlabel('Relevant words index')\n",
    "        #plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "        plt.show()\n",
    "        for cv in self.orderedCVList[int(len(self.orderedCVList)/2)-1:int(len(self.orderedCVList)/2)+1]:\n",
    "            xValue = np.arange(len(cv[cvIndex].frequencyVector))\n",
    "            plt.plot(xValue,cv[cvIndex].frequencyVector, markersize = 10,label=cv[cvIndex].fileName+'='+str(cv[cvIndex].score))\n",
    "        plt.title('CV comparision middle CV')\n",
    "        plt.ylabel('Feature Vector Frequency')\n",
    "        plt.legend(mode=\"expand\")\n",
    "        plt.xlabel('Relevant words index')\n",
    "        #plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "        plt.show()\n",
    "        for cv in self.orderedCVList[-2:]:\n",
    "            xValue = np.arange(len(cv[cvIndex].frequencyVector))\n",
    "            plt.plot(xValue,cv[cvIndex].frequencyVector, markersize = 10,label=cv[cvIndex].fileName+'='+str(cv[cvIndex].score))\n",
    "        plt.title('CV comparision last CV')\n",
    "        plt.ylabel('Feature Vector Frequency')\n",
    "        plt.legend(mode=\"expand\")\n",
    "        plt.xlabel('Relevant words index')\n",
    "        #plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "        plt.show()\n",
    "       \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class communicationInformation:\n",
    "    def __init__(self):\n",
    "        self.directoryPath=\"\"\n",
    "        self.relevantWords=[]\n",
    "        self.jobSelected=\"\"\n",
    "        self.workFlow=True\n",
    "        relevantWords1=[\n",
    "      'css',\n",
    "   'design',\n",
    "  'html',\n",
    "  'javascript',\n",
    " 'jquery',\n",
    " 'mysql',\n",
    "  'ajax',\n",
    "     'php',\n",
    "    'animation','adobe','flash','character','art','illustrator','design','animator','effects','maya','photoshop',\n",
    "    \n",
    "   \"software\",\"skills\",\"application\",\"developer\",\"server\",\n",
    "   \"systems\",\"framework\",\"net\",\"visual\",\n",
    "    \n",
    "     \"algorithm\",\"analyst\",\"aws\",\"datasets\",\"clustering\",\"intelligence\",\"logistic\",\"mining\",\"neural\",\"regression\",\"scikit\"\n",
    "\n",
    "]\n",
    "        self.relevantWords=list(set(relevantWords1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import random\n",
    "class buttonHandler:\n",
    "    def __init__(self, master,buttonDataList):\n",
    "        self.buttonList=[]\n",
    "        self.frame = Frame(master)\n",
    "        self.frame.pack()\n",
    "        vhdlWork=[]\n",
    "        f=open(\"F:\\\\VHDL\\\\text.txt\",\"w\")\n",
    "        for buttonName in buttonDataList:\n",
    "                buttonToShow=Button(self.frame,text=str(buttonName[0].fileName),command=lambda:self.openPdf(buttonName[0].filePath))\n",
    "                #f.write(\n",
    "                value=str(str(buttonName[0].fileName[:10])+\" \"+str(int(buttonName[1]))+\"\\n\")\n",
    "                vhdlWork.append(value)\n",
    "                buttonToShow.pack(padx=5,pady=10)\n",
    "                self.buttonList.append(buttonToShow)\n",
    "        random.shuffle(vhdlWork)\n",
    "        for eachLine in vhdlWork:\n",
    "            f.write(eachLine)\n",
    "        f.close()\n",
    "        print(vhdlWork)\n",
    "    def openPdf(self,link):\n",
    "        webbrowser.open_new_tab(link) \n",
    "    def destoringButtons(self):\n",
    "        self.frame.destroy()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tkinter import*\n",
    "import tkinter.filedialog \n",
    "from tkinter import Tk, StringVar, ttk\n",
    "import webbrowser\n",
    "\n",
    "class GUI:\n",
    "    def __init__(self,root):\n",
    "        self.passingInfo=communicationInformation()\n",
    "        self.root=root\n",
    "        self.run=False\n",
    "        self.frame = Frame()\n",
    "        self.frame.pack(fill=X)\n",
    "        self.directoryLabel = Label(self.frame,text=\"Path\",  width=10)\n",
    "        self.directoryLabel.pack(side=LEFT)\n",
    "        self.directoryEntry = Entry(self.frame,width=100)\n",
    "        self.directoryEntry.pack(side=LEFT, padx=0,pady=10 ,expand=True)\n",
    "        self.addDirectoryButton = Button(self.root, text =\"     Add    \",command=self.pathAdd)\n",
    "        self.addDirectoryButton.pack(padx=5,pady=10)\n",
    "        self.buttonList=None\n",
    "        self.jobReqFrame = Frame()\n",
    "        self.jobReqFrame.pack(side=LEFT,fill=X)        \n",
    "        self.jobRequirementLabel  = Label(self.jobReqFrame, text=\"Jobs Requirements\", width=50)\n",
    "        self.jobRequirementLabel.pack(expand=True, padx=5, pady=5)     \n",
    "        self.relevantWordsText = Text(self.jobReqFrame,width=40,height=15)\n",
    "        self.relevantWordsText.pack( side=LEFT,pady=5, padx=5)\n",
    "        self.relevantWordsText.insert('1.0',' '.join(self.passingInfo.relevantWords))\n",
    "        self.CVTitleFrame=Frame()\n",
    "        self.CVTitleFrame.pack(fill=X)\n",
    "        self.CVTitleLabel = Label(self.CVTitleFrame, text=\"Selecte Post\", width=50)\n",
    "        self.CVTitleLabel.pack( anchor=N, padx=0, pady=0)\n",
    "        \n",
    "        self.topCVDisplay = Frame()\n",
    "        self.topCVDisplay.pack(fill=X,padx=50)\n",
    "        self.box_value = StringVar()\n",
    "        self.box = ttk.Combobox(self.topCVDisplay, textvariable=self.box_value,state='readonly')\n",
    "        self.box['values'] = ( 'animator','webDeveloper','SoftwareDeveloper','')\n",
    "        self.box.grid(column=0, row=0)\n",
    "        #process button\n",
    "        self.processButton = Button(self.jobReqFrame, text =\"    Process    \",command=self.processExe)\n",
    "        self.processButton.pack(side=LEFT,padx=10,pady=10)\n",
    "        self.analyze=None\n",
    "        self.exitButton = Button(self.root, text =\"    Exit    \",command=self.root.destroy)\n",
    "        self.exitButton.pack(side=LEFT,padx=10,pady=10)\n",
    "        self.manager=CVManager()\n",
    "\n",
    "    def pathAdd(self):\n",
    "        directoryPath=filedialog.askdirectory()\n",
    "        self.directoryEntry.insert(0, directoryPath)\n",
    "        \n",
    "    def processExe(self):\n",
    "        if(not(self.run)):\n",
    "            self.passingInfo.directoryPath=self.directoryEntry.get()\n",
    "            self.passingInfo.jobSelected=self.box.get()\n",
    "            if self.passingInfo.workFlow:   \n",
    "                try:\n",
    "                    self.run=True\n",
    "                    self.manager.list_CVs(self.passingInfo.directoryPath)\n",
    "                    self.manager.collectCV()\n",
    "                    self.manager.findDocumentMatrix(None,self.passingInfo.relevantWords)\n",
    "                    self.manager.clusterData()\n",
    "                    self.manager.rankCV()\n",
    "                    self.manager.showAnalytics()\n",
    "                    self.manager.orderedCVList=self.manager.showTopCVPerPost(self.passingInfo.jobSelected)\n",
    "                    self.analyze = Button(self.root, text =\"analyze\",command=self.analyzeCV)\n",
    "                    self.analyze.pack(side=LEFT,padx=10,pady=10)\n",
    "                    self.createLinkToCV(self.manager.orderedCVList)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"error\")\n",
    "                    print(\"processing \\t\"+str(e))\n",
    "            else:\n",
    "                print(\"select all necessary info\")\n",
    "        else:\n",
    "            try:\n",
    "                self.passingInfo.jobSelected=self.box.get()\n",
    "                self.manager.orderedCVList=self.manager.showTopCVPerPost(self.passingInfo.jobSelected)\n",
    "                self.buttonList.destoringButtons()\n",
    "                self.createLinkToCV(self.manager.orderedCVList)\n",
    "            except Exception as e:\n",
    "                    print(\"error\")\n",
    "                    print(\"processing second time \\t\"+str(e))\n",
    "    def createLinkToCV(self,dataList):\n",
    "        self.buttonList=buttonHandler(self.root,dataList)\n",
    "    def analyzeCV(self):\n",
    "        self.manager.compareCV()\n",
    "        \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root=Tk()\n",
    "graphics=GUI(root)\n",
    "root.geometry(\"700x600\")\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
